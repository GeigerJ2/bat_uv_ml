{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d96feaf-8817-453a-b4d0-560f5f29d33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profile<uuid='4fe00b7a32994cfca8b6cf43c2d22a53' name='jgeiger'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "from aiida import orm, load_profile\n",
    "from aiida.engine import submit\n",
    "\n",
    "from qe_tools import CONSTANTS\n",
    "\n",
    "from aiida_quantumespresso_hp.workflows.hubbard import SelfConsistentHubbardWorkChain\n",
    "from aiida_quantumespresso.data.hubbard_structure import HubbardStructureData\n",
    "\n",
    "load_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26411ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22mStarting the daemon with 4 workers... \u001b[0m\u001b[32m\u001b[1mRUNNING\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! verdi daemon start 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b09cc3-665b-43e6-b04b-2793d534dc7f",
   "metadata": {},
   "source": [
    "# BaTiO3\n",
    "Let's define the BTO structure to use as example for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafa590d-b1de-4a92-a730-5c6df3d717c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = [\n",
    "    [5.34198, -3.08419, 4.38073],\n",
    "    [0.00000,  6.16839, 4.38073],\n",
    "    [-5.3419, -3.08419, 4.38073],\n",
    "]\n",
    "cell = CONSTANTS.bohr_to_ang*np.array(cell)\n",
    "\n",
    "structure = orm.StructureData(cell=cell)\n",
    "\n",
    "structure.append_atom(position=np.dot([0.48674, 0.4867, 0.4867], cell),  symbols='Ti')\n",
    "structure.append_atom(position=np.dot([0.51163, 0.5116, 0.0192], cell),  symbols='O' )\n",
    "structure.append_atom(position=np.dot([0.51163, 0.0192, 0.5116], cell),  symbols='O' )\n",
    "structure.append_atom(position=np.dot([0.01929, 0.5116, 0.5116], cell),  symbols='O' )\n",
    "structure.append_atom(position=np.dot([0.99899, 0.9989, 0.9989], cell),  symbols='Ba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6c351-07bf-4935-8149-c5dc3e802ca2",
   "metadata": {},
   "source": [
    "## HubbardStructureData initialization\n",
    "Let's initialize the HubbardStructureData with the BTO structure!\n",
    "\n",
    "> Important: You only need to do these steps once! Later just keep track of this testing structure via the PK and reload it from the database so you don't get 100 BaTiO3 strucures in your storage. ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68238387-c713-47b0-b501-28f6cee06b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard_data = HubbardStructureData(structure=structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd1cbca-7de7-483e-a7a6-8b7aa900254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HubbardStructureData: uuid: 121783bc-8579-4b3a-986b-2740cc20fec0 (unstored)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubbard_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3586b0-0d08-437c-9416-28b4de12ad7e",
   "metadata": {},
   "source": [
    "## Initializing the on-site Hubbard\n",
    "Let's initialize the on-site Hubbard parameter for the titanium atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97aa36f8-e5b9-435c-b4bd-08c4850b02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard_data.initialize_onsites_hubbard('Ti', '3d', 5.0) # index 0 for Ti ==> Hubbard U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271f32e-df00-49f8-a678-d1f83d51bae1",
   "metadata": {},
   "source": [
    "Here how it is stored in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bc5921-ce79-4e3d-804e-3ed6523e6b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '3d', 0, '3d', 5.0, [0, 0, 0], 'dudarev']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubbard_data.hubbard_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97632bc2-b652-4d04-b917-15c688468253",
   "metadata": {},
   "source": [
    "## Initializing the inter-site Hubbard\n",
    "Let's initialize the inter-site Hubbard parameter for the titanium and oxygen atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb86220-436e-4592-af95-a359c8923b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard_data.initialize_intersites_hubbard('Ti', '3d', 'O', '2p', 0.0001, number_of_neighbours=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df15af-5574-42be-aa58-890d2d95c5d2",
   "metadata": {},
   "source": [
    "The parameters are saved in the property `hubbard_parameters` as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab486809-31f7-4bc3-9425-0467811da1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '3d', 0, '3d', 5.0, [0, 0, 0], 'dudarev'],\n",
       " [0, '3d', 3, '2p', 0.0001, [0, 0, 0], 'dudarev'],\n",
       " [0, '3d', 1, '2p', 0.0001, [0, 0, 0], 'dudarev'],\n",
       " [0, '3d', 2, '2p', 0.0001, [0, 0, 0], 'dudarev'],\n",
       " [0, '3d', 1, '2p', 0.0001, [0, 0, 1], 'dudarev'],\n",
       " [0, '3d', 2, '2p', 0.0001, [0, 1, 0], 'dudarev'],\n",
       " [0, '3d', 3, '2p', 0.0001, [1, 0, 0], 'dudarev'],\n",
       " [0, '3d', 3, '2p', 0.0001, [0, 0, -1], 'dudarev']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubbard_data.hubbard_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e83583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUBBARD\tortho-atomic\n",
      " U\tTi-3d\t5.0\n",
      " V\tTi-3d\tO-2p\t1\t4\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t2\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t3\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t72\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t83\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t114\t0.0001\n",
      " V\tTi-3d\tO-2p\t1\t69\t0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hubbard_data.get_quantum_espresso_hubbard_card())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb1b4b3b-8fe5-4f29-8ee9-54e3f564017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HubbardStructureData: uuid: 121783bc-8579-4b3a-986b-2740cc20fec0 (pk: 2502)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubbard_data.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c370c-8f7c-401b-93f6-77c51225d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard_data.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd55280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# verdi data show 2184"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e706b-5f5c-4f85-92aa-45355eb17e89",
   "metadata": {},
   "source": [
    "## Running the `SelfConsistentHubbardWorkChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard_data = orm.load_node(2502)  # I load the node from the database instead of always regenerating it\n",
    "# type(hubbard_data)\n",
    "# from aiida_quantumespresso.data.hubbard_structure import HubbardStructureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_code = orm.load_code('qe-dev-pw@lumi-small')\n",
    "hp_code = orm.load_code('qe-dev-hp@lumi-small')\n",
    "\n",
    "builder = SelfConsistentHubbardWorkChain.get_builder_from_protocol(\n",
    "    pw_code=pw_code,\n",
    "    hp_code=hp_code,\n",
    "    hubbard_structure=hubbard_data,\n",
    "    protocol='fast',\n",
    "    overrides=pathlib.Path('hubbard_overrides.yaml')\n",
    ")\n",
    "\n",
    "builder.skip_first_relax = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc968bdc-0817-49a0-8e89-e14323248b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_submit = submit(builder)\n",
    "# print(second_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_submit_pk = 2234\n",
    "second_submit_pk = 2448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fac19fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property     Value\n",
      "-----------  ---------------------------------------------------------------------\n",
      "type         SelfConsistentHubbardWorkChain\n",
      "state        Finished [402] The PwRelaxWorkChain sub process failed in iteration 2\n",
      "pk           2448\n",
      "uuid         767c3be7-8042-4e79-9fe6-8ec2af85655e\n",
      "label\n",
      "description\n",
      "ctime        2023-02-21 14:30:56.614402+01:00\n",
      "mtime        2023-02-21 15:22:09.913790+01:00\n",
      "\n",
      "Inputs                               PK    Type\n",
      "-----------------------------------  ----  --------------------\n",
      "hubbard\n",
      "    hp\n",
      "        code                         2183  InstalledCode\n",
      "        qpoints                      2437  KpointsData\n",
      "        parameters                   2438  Dict\n",
      "        settings                     2439  Dict\n",
      "    clean_workdir                    2440  Bool\n",
      "    max_iterations                   2441  Int\n",
      "    parallelize_atoms                2442  Bool\n",
      "relax\n",
      "    base\n",
      "        pw\n",
      "            pseudos\n",
      "                Ti                   14    UpfData\n",
      "                O                    74    UpfData\n",
      "                Ba                   45    UpfData\n",
      "            code                     2182  InstalledCode\n",
      "            parameters               2424  Dict\n",
      "        kpoints_distance             2425  Float\n",
      "        kpoints_force_parity         2426  Bool\n",
      "        max_iterations               2427  Int\n",
      "    clean_workdir                    2428  Bool\n",
      "    max_meta_convergence_iterations  2429  Int\n",
      "    meta_convergence                 2430  Bool\n",
      "    volume_convergence               2431  Float\n",
      "scf\n",
      "    pw\n",
      "        pseudos\n",
      "            Ti                       14    UpfData\n",
      "            O                        74    UpfData\n",
      "            Ba                       45    UpfData\n",
      "        code                         2182  InstalledCode\n",
      "        parameters                   2432  Dict\n",
      "    clean_workdir                    2433  Bool\n",
      "    kpoints_distance                 2434  Float\n",
      "    kpoints_force_parity             2435  Bool\n",
      "    max_iterations                   2436  Int\n",
      "clean_workdir                        2446  Bool\n",
      "hubbard_structure                    2184  HubbardStructureData\n",
      "max_iterations                       2447  Int\n",
      "meta_convergence                     2445  Bool\n",
      "tolerance_intersite                  2444  Float\n",
      "tolerance_onsite                     2443  Float\n",
      "\n",
      "Called                       PK  Type\n",
      "-------------------------  ----  -----------------------\n",
      "iteration_01_scf_smearing  2450  PwBaseWorkChain\n",
      "iteration_01_scf_fixed     2462  PwBaseWorkChain\n",
      "iteration_01_hp            2473  HpWorkChain\n",
      "CALL                       2485  structure_relabel_kinds\n",
      "iteration_02_relax         2488  PwRelaxWorkChain\n",
      "\n",
      "Log messages\n",
      "----------------------------------------------\n",
      "There are 11 log messages for this calculation\n",
      "Run 'verdi process report 2448' to see them\n",
      "====================================================================================================\n",
      "SelfConsistentHubbardWorkChain<2448> Finished [402] [1:while_(should_run_iteration)(1:if_(should_run_relax)(1:inspect_relax))]\n",
      "    ├── PwBaseWorkChain<2450> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2451> Finished [0]\n",
      "    │   └── PwCalculation<2455> Finished [0]\n",
      "    ├── PwBaseWorkChain<2462> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2463> Finished [0]\n",
      "    │   └── PwCalculation<2467> Finished [0]\n",
      "    ├── HpWorkChain<2473> Finished [0] [2:results]\n",
      "    │   └── HpBaseWorkChain<2475> Finished [0] [3:results]\n",
      "    │       └── HpCalculation<2477> Finished [0]\n",
      "    ├── structure_relabel_kinds<2485> Finished [0]\n",
      "    └── PwRelaxWorkChain<2488> Finished [401] [1:while_(should_run_relax)(1:inspect_relax)]\n",
      "        └── PwBaseWorkChain<2491> Finished [300] [3:while_(should_run_process)(2:inspect_process)]\n",
      "            ├── create_kpoints_from_distance<2492> Finished [0]\n",
      "            └── PwCalculation<2496> Finished [305]\n",
      "====================================================================================================\n",
      "2023-02-21 14:31:00 [109 | REPORT]: [2448|SelfConsistentHubbardWorkChain|setup]: system is treated to be non-magnetic because `nspin == 1` in `scf.pw.parameters` input.\n",
      "2023-02-21 14:31:00 [110 | REPORT]: [2448|SelfConsistentHubbardWorkChain|should_run_relax]: skip_first_relax has been set True; skipping first relaxion...\n",
      "2023-02-21 14:31:01 [111 | REPORT]: [2448|SelfConsistentHubbardWorkChain|run_scf_smearing]: launching PwBaseWorkChain<2450> with smeared occupations\n",
      "2023-02-21 14:31:04 [112 | REPORT]:   [2450|PwBaseWorkChain|run_process]: launching PwCalculation<2455> iteration #1\n",
      "2023-02-21 14:40:36 [113 | REPORT]:   [2450|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-21 14:40:37 [114 | REPORT]:   [2450|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-21 14:40:37 [115 | REPORT]: [2448|SelfConsistentHubbardWorkChain|recon_scf]: after relaxation, system is determined to be an insulator\n",
      "2023-02-21 14:40:38 [116 | REPORT]: [2448|SelfConsistentHubbardWorkChain|run_scf_fixed]: launching PwBaseWorkChain<2462> with fixed occupations\n",
      "2023-02-21 14:40:41 [117 | REPORT]:   [2462|PwBaseWorkChain|run_process]: launching PwCalculation<2467> iteration #1\n",
      "2023-02-21 14:57:28 [118 | REPORT]:   [2462|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-21 14:57:28 [119 | REPORT]:   [2462|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-21 14:57:29 [120 | REPORT]: [2448|SelfConsistentHubbardWorkChain|run_hp]: launching HpWorkChain<2473> iteration #1\n",
      "2023-02-21 14:57:30 [121 | REPORT]:   [2473|HpWorkChain|run_base_workchain]: running in serial, launching HpBaseWorkChain<2475>\n",
      "2023-02-21 14:57:32 [122 | REPORT]:     [2475|HpBaseWorkChain|run_process]: launching HpCalculation<2477> iteration #1\n",
      "2023-02-21 15:11:47 [123 | REPORT]:     [2475|HpBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-21 15:11:48 [124 | REPORT]:     [2475|HpBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-21 15:11:49 [125 | REPORT]: [2448|SelfConsistentHubbardWorkChain|check_convergence]: new types have been determined: relabeling the structure and starting new iteration.\n",
      "2023-02-21 15:11:49 [126 | REPORT]: [2448|SelfConsistentHubbardWorkChain|check_convergence]: The new and old Hubbard parameters have different lenghts. Assuming to be at the first cycle.\n",
      "2023-02-21 15:11:50 [127 | REPORT]: [2448|SelfConsistentHubbardWorkChain|run_relax]: launching PwRelaxWorkChain<2488> iteration #2\n",
      "2023-02-21 15:11:52 [128 | REPORT]:   [2488|PwRelaxWorkChain|run_relax]: launching PwBaseWorkChain<2491>\n",
      "2023-02-21 15:11:53 [129 | REPORT]:     [2491|PwBaseWorkChain|run_process]: launching PwCalculation<2496> iteration #1\n",
      "2023-02-21 15:22:08 [134 | REPORT]:     [2491|PwBaseWorkChain|report_error_handled]: PwCalculation<2496> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-21 15:22:08 [135 | REPORT]:     [2491|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-21 15:22:09 [136 | REPORT]:     [2491|PwBaseWorkChain|inspect_process]: PwCalculation<2496> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-21 15:22:09 [137 | REPORT]:     [2491|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-21 15:22:09 [138 | REPORT]:   [2488|PwRelaxWorkChain|inspect_relax]: relax PwBaseWorkChain failed with exit status 300\n",
      "2023-02-21 15:22:09 [139 | REPORT]:   [2488|PwRelaxWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-21 15:22:09 [140 | REPORT]: [2448|SelfConsistentHubbardWorkChain|inspect_relax]: PwRelaxWorkChain failed with exit status 401\n",
      "2023-02-21 15:22:09 [141 | REPORT]: [2448|SelfConsistentHubbardWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "verdi process show 2448\n",
    "printf '=%.0s' {1..100}\n",
    "echo\n",
    "verdi process status 2448\n",
    "printf '=%.0s' {1..100}\n",
    "echo\n",
    "verdi process report 2448\n",
    "printf '=%.0s' {1..100}\n",
    "echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b99da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property     Value\n",
      "-----------  ---------------------------------------------------------------------\n",
      "type         SelfConsistentHubbardWorkChain\n",
      "state        Finished [402] The PwRelaxWorkChain sub process failed in iteration 2\n",
      "pk           2234\n",
      "uuid         8a7af431-03dd-44f4-a84c-a17e55ead812\n",
      "label\n",
      "description\n",
      "ctime        2023-02-16 10:03:47.493781+01:00\n",
      "mtime        2023-02-16 10:21:15.871575+01:00\n",
      "\n",
      "Inputs                               PK    Type\n",
      "-----------------------------------  ----  --------------------\n",
      "hubbard\n",
      "    hp\n",
      "        code                         2183  InstalledCode\n",
      "        qpoints                      2223  KpointsData\n",
      "        parameters                   2224  Dict\n",
      "        settings                     2225  Dict\n",
      "    clean_workdir                    2226  Bool\n",
      "    max_iterations                   2227  Int\n",
      "    parallelize_atoms                2228  Bool\n",
      "relax\n",
      "    base\n",
      "        pw\n",
      "            pseudos\n",
      "                Ti                   14    UpfData\n",
      "                O                    74    UpfData\n",
      "                Ba                   45    UpfData\n",
      "            code                     2182  InstalledCode\n",
      "            parameters               2210  Dict\n",
      "        kpoints_distance             2211  Float\n",
      "        kpoints_force_parity         2212  Bool\n",
      "        max_iterations               2213  Int\n",
      "    clean_workdir                    2214  Bool\n",
      "    max_meta_convergence_iterations  2215  Int\n",
      "    meta_convergence                 2216  Bool\n",
      "    volume_convergence               2217  Float\n",
      "scf\n",
      "    pw\n",
      "        pseudos\n",
      "            Ti                       14    UpfData\n",
      "            O                        74    UpfData\n",
      "            Ba                       45    UpfData\n",
      "        code                         2182  InstalledCode\n",
      "        parameters                   2218  Dict\n",
      "    clean_workdir                    2219  Bool\n",
      "    kpoints_distance                 2220  Float\n",
      "    kpoints_force_parity             2221  Bool\n",
      "    max_iterations                   2222  Int\n",
      "clean_workdir                        2232  Bool\n",
      "hubbard_structure                    2184  HubbardStructureData\n",
      "max_iterations                       2233  Int\n",
      "meta_convergence                     2231  Bool\n",
      "tolerance_intersite                  2230  Float\n",
      "tolerance_onsite                     2229  Float\n",
      "\n",
      "Called                       PK  Type\n",
      "-------------------------  ----  -----------------------\n",
      "iteration_01_scf_smearing  2236  PwBaseWorkChain\n",
      "iteration_01_scf_fixed     2261  PwBaseWorkChain\n",
      "iteration_01_hp            2284  HpWorkChain\n",
      "CALL                       2307  structure_relabel_kinds\n",
      "iteration_02_relax         2310  PwRelaxWorkChain\n",
      "\n",
      "Log messages\n",
      "----------------------------------------------\n",
      "There are 11 log messages for this calculation\n",
      "Run 'verdi process report 2234' to see them\n",
      "SelfConsistentHubbardWorkChain<2234> Finished [402] [1:while_(should_run_iteration)(1:if_(should_run_relax)(1:inspect_relax))]\n",
      "    ├── PwBaseWorkChain<2236> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2237> Finished [0]\n",
      "    │   └── PwCalculation<2241> Finished [0]\n",
      "    ├── PwBaseWorkChain<2261> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2262> Finished [0]\n",
      "    │   └── PwCalculation<2266> Finished [0]\n",
      "    ├── HpWorkChain<2284> Finished [0] [2:results]\n",
      "    │   └── HpBaseWorkChain<2286> Finished [0] [3:results]\n",
      "    │       └── HpCalculation<2288> Finished [0]\n",
      "    ├── structure_relabel_kinds<2307> Finished [0]\n",
      "    └── PwRelaxWorkChain<2310> Finished [401] [1:while_(should_run_relax)(1:inspect_relax)]\n",
      "        └── PwBaseWorkChain<2313> Finished [300] [3:while_(should_run_process)(2:inspect_process)]\n",
      "            ├── create_kpoints_from_distance<2314> Finished [0]\n",
      "            └── PwCalculation<2318> Finished [305]\n",
      "2023-02-16 10:03:50 [18 | REPORT]: [2234|SelfConsistentHubbardWorkChain|setup]: system is treated to be non-magnetic because `nspin == 1` in `scf.pw.parameters` input.\n",
      "2023-02-16 10:03:51 [19 | REPORT]: [2234|SelfConsistentHubbardWorkChain|should_run_relax]: skip_first_relax has been set True; skipping first relaxion...\n",
      "2023-02-16 10:03:51 [20 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_scf_smearing]: launching PwBaseWorkChain<2236> with smeared occupations\n",
      "2023-02-16 10:03:54 [21 | REPORT]:   [2236|PwBaseWorkChain|run_process]: launching PwCalculation<2241> iteration #1\n",
      "2023-02-16 10:06:21 [26 | REPORT]:   [2236|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:06:21 [27 | REPORT]:   [2236|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:06:21 [28 | REPORT]: [2234|SelfConsistentHubbardWorkChain|recon_scf]: after relaxation, system is determined to be an insulator\n",
      "2023-02-16 10:06:22 [29 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_scf_fixed]: launching PwBaseWorkChain<2261> with fixed occupations\n",
      "2023-02-16 10:06:25 [30 | REPORT]:   [2261|PwBaseWorkChain|run_process]: launching PwCalculation<2266> iteration #1\n",
      "2023-02-16 10:11:34 [36 | REPORT]:   [2261|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:11:34 [37 | REPORT]:   [2261|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:11:35 [38 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_hp]: launching HpWorkChain<2284> iteration #1\n",
      "2023-02-16 10:11:36 [39 | REPORT]:   [2284|HpWorkChain|run_base_workchain]: running in serial, launching HpBaseWorkChain<2286>\n",
      "2023-02-16 10:11:36 [40 | REPORT]:     [2286|HpBaseWorkChain|run_process]: launching HpCalculation<2288> iteration #1\n",
      "2023-02-16 10:18:47 [46 | REPORT]:     [2286|HpBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:18:47 [47 | REPORT]:     [2286|HpBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:18:48 [48 | REPORT]: [2234|SelfConsistentHubbardWorkChain|check_convergence]: new types have been determined: relabeling the structure and starting new iteration.\n",
      "2023-02-16 10:18:49 [49 | REPORT]: [2234|SelfConsistentHubbardWorkChain|check_convergence]: The new and old Hubbard parameters have different lenghts. Assuming to be at the first cycle.\n",
      "2023-02-16 10:18:50 [50 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_relax]: launching PwRelaxWorkChain<2310> iteration #2\n",
      "2023-02-16 10:18:51 [51 | REPORT]:   [2310|PwRelaxWorkChain|run_relax]: launching PwBaseWorkChain<2313>\n",
      "2023-02-16 10:18:52 [52 | REPORT]:     [2313|PwBaseWorkChain|run_process]: launching PwCalculation<2318> iteration #1\n",
      "2023-02-16 10:21:15 [57 | REPORT]:     [2313|PwBaseWorkChain|report_error_handled]: PwCalculation<2318> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-16 10:21:15 [58 | REPORT]:     [2313|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-16 10:21:15 [59 | REPORT]:     [2313|PwBaseWorkChain|inspect_process]: PwCalculation<2318> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-16 10:21:15 [60 | REPORT]:     [2313|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:21:15 [61 | REPORT]:   [2310|PwRelaxWorkChain|inspect_relax]: relax PwBaseWorkChain failed with exit status 300\n",
      "2023-02-16 10:21:15 [62 | REPORT]:   [2310|PwRelaxWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:21:15 [63 | REPORT]: [2234|SelfConsistentHubbardWorkChain|inspect_relax]: PwRelaxWorkChain failed with exit status 401\n",
      "2023-02-16 10:21:15 [64 | REPORT]: [2234|SelfConsistentHubbardWorkChain|on_terminated]: remote folders will not be cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: repchar: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "verdi process show 2234\n",
    "verdi process status 2234\n",
    "verdi process report 2234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "259ae70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 10:18:52 [52 | REPORT]: [2313|PwBaseWorkChain|run_process]: launching PwCalculation<2318> iteration #1\n",
      "2023-02-16 10:21:15 [57 | REPORT]: [2313|PwBaseWorkChain|report_error_handled]: PwCalculation<2318> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-16 10:21:15 [58 | REPORT]: [2313|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-16 10:21:15 [59 | REPORT]: [2313|PwBaseWorkChain|inspect_process]: PwCalculation<2318> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-16 10:21:15 [60 | REPORT]: [2313|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "verdi process report 2313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2b7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1d1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22m2023-02-16 10:18:51 [51 | REPORT]: [2310|PwRelaxWorkChain|run_relax]: launching PwBaseWorkChain<2313>\n",
      "2023-02-16 10:18:52 [52 | REPORT]:   [2313|PwBaseWorkChain|run_process]: launching PwCalculation<2318> iteration #1\n",
      "2023-02-16 10:21:15 [57 | REPORT]:   [2313|PwBaseWorkChain|report_error_handled]: PwCalculation<2318> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-16 10:21:15 [58 | REPORT]:   [2313|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-16 10:21:15 [59 | REPORT]:   [2313|PwBaseWorkChain|inspect_process]: PwCalculation<2318> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-16 10:21:15 [60 | REPORT]:   [2313|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:21:15 [61 | REPORT]: [2310|PwRelaxWorkChain|inspect_relax]: relax PwBaseWorkChain failed with exit status 300\n",
      "2023-02-16 10:21:15 [62 | REPORT]: [2310|PwRelaxWorkChain|on_terminated]: remote folders will not be cleaned\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!verdi process report 2310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f13c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22m2023-02-16 10:18:52 [52 | REPORT]: [2313|PwBaseWorkChain|run_process]: launching PwCalculation<2318> iteration #1\n",
      "2023-02-16 10:21:15 [57 | REPORT]: [2313|PwBaseWorkChain|report_error_handled]: PwCalculation<2318> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-16 10:21:15 [58 | REPORT]: [2313|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-16 10:21:15 [59 | REPORT]: [2313|PwBaseWorkChain|inspect_process]: PwCalculation<2318> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-16 10:21:15 [60 | REPORT]: [2313|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!verdi process report 2313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022f9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22m2023-02-16 10:03:50 [18 | REPORT]: [2234|SelfConsistentHubbardWorkChain|setup]: system is treated to be non-magnetic because `nspin == 1` in `scf.pw.parameters` input.\n",
      "2023-02-16 10:03:51 [19 | REPORT]: [2234|SelfConsistentHubbardWorkChain|should_run_relax]: skip_first_relax has been set True; skipping first relaxion...\n",
      "2023-02-16 10:03:51 [20 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_scf_smearing]: launching PwBaseWorkChain<2236> with smeared occupations\n",
      "2023-02-16 10:03:54 [21 | REPORT]:   [2236|PwBaseWorkChain|run_process]: launching PwCalculation<2241> iteration #1\n",
      "2023-02-16 10:06:21 [26 | REPORT]:   [2236|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:06:21 [27 | REPORT]:   [2236|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:06:21 [28 | REPORT]: [2234|SelfConsistentHubbardWorkChain|recon_scf]: after relaxation, system is determined to be an insulator\n",
      "2023-02-16 10:06:22 [29 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_scf_fixed]: launching PwBaseWorkChain<2261> with fixed occupations\n",
      "2023-02-16 10:06:25 [30 | REPORT]:   [2261|PwBaseWorkChain|run_process]: launching PwCalculation<2266> iteration #1\n",
      "2023-02-16 10:11:34 [36 | REPORT]:   [2261|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:11:34 [37 | REPORT]:   [2261|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:11:35 [38 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_hp]: launching HpWorkChain<2284> iteration #1\n",
      "2023-02-16 10:11:36 [39 | REPORT]:   [2284|HpWorkChain|run_base_workchain]: running in serial, launching HpBaseWorkChain<2286>\n",
      "2023-02-16 10:11:36 [40 | REPORT]:     [2286|HpBaseWorkChain|run_process]: launching HpCalculation<2288> iteration #1\n",
      "2023-02-16 10:18:47 [46 | REPORT]:     [2286|HpBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2023-02-16 10:18:47 [47 | REPORT]:     [2286|HpBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:18:48 [48 | REPORT]: [2234|SelfConsistentHubbardWorkChain|check_convergence]: new types have been determined: relabeling the structure and starting new iteration.\n",
      "2023-02-16 10:18:49 [49 | REPORT]: [2234|SelfConsistentHubbardWorkChain|check_convergence]: The new and old Hubbard parameters have different lenghts. Assuming to be at the first cycle.\n",
      "2023-02-16 10:18:50 [50 | REPORT]: [2234|SelfConsistentHubbardWorkChain|run_relax]: launching PwRelaxWorkChain<2310> iteration #2\n",
      "2023-02-16 10:18:51 [51 | REPORT]:   [2310|PwRelaxWorkChain|run_relax]: launching PwBaseWorkChain<2313>\n",
      "2023-02-16 10:18:52 [52 | REPORT]:     [2313|PwBaseWorkChain|run_process]: launching PwCalculation<2318> iteration #1\n",
      "2023-02-16 10:21:15 [57 | REPORT]:     [2313|PwBaseWorkChain|report_error_handled]: PwCalculation<2318> failed with exit status 305: Both the stdout and XML output files could not be read or parsed.\n",
      "2023-02-16 10:21:15 [58 | REPORT]:     [2313|PwBaseWorkChain|report_error_handled]: Action taken: unrecoverable error, aborting...\n",
      "2023-02-16 10:21:15 [59 | REPORT]:     [2313|PwBaseWorkChain|inspect_process]: PwCalculation<2318> failed but a handler detected an unrecoverable problem, aborting\n",
      "2023-02-16 10:21:15 [60 | REPORT]:     [2313|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:21:15 [61 | REPORT]:   [2310|PwRelaxWorkChain|inspect_relax]: relax PwBaseWorkChain failed with exit status 300\n",
      "2023-02-16 10:21:15 [62 | REPORT]:   [2310|PwRelaxWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2023-02-16 10:21:15 [63 | REPORT]: [2234|SelfConsistentHubbardWorkChain|inspect_relax]: PwRelaxWorkChain failed with exit status 401\n",
      "2023-02-16 10:21:15 [64 | REPORT]: [2234|SelfConsistentHubbardWorkChain|on_terminated]: remote folders will not be cleaned\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!verdi process report 2234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbb7fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22mSelfConsistentHubbardWorkChain<2234> Finished [402] [1:while_(should_run_iteration)(1:if_(should_run_relax)(1:inspect_relax))]\n",
      "    ├── PwBaseWorkChain<2236> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2237> Finished [0]\n",
      "    │   └── PwCalculation<2241> Finished [0]\n",
      "    ├── PwBaseWorkChain<2261> Finished [0] [4:results]\n",
      "    │   ├── create_kpoints_from_distance<2262> Finished [0]\n",
      "    │   └── PwCalculation<2266> Finished [0]\n",
      "    ├── HpWorkChain<2284> Finished [0] [2:results]\n",
      "    │   └── HpBaseWorkChain<2286> Finished [0] [3:results]\n",
      "    │       └── HpCalculation<2288> Finished [0]\n",
      "    ├── structure_relabel_kinds<2307> Finished [0]\n",
      "    └── PwRelaxWorkChain<2310> Finished [401] [1:while_(should_run_relax)(1:inspect_relax)]\n",
      "        └── PwBaseWorkChain<2313> Finished [300] [3:while_(should_run_process)(2:inspect_process)]\n",
      "            ├── create_kpoints_from_distance<2314> Finished [0]\n",
      "            └── PwCalculation<2318> Finished [305]\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcd6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 2318: None\n",
      "*** (empty scheduler output file)\n",
      "*** Scheduler errors:\n",
      "\n",
      "Lmod is automatically replacing \"cce/14.0.2\" with \"gcc/11.2.0\".\n",
      "\n",
      "\n",
      "Lmod is automatically replacing \"PrgEnv-cray/8.3.3\" with \"PrgEnv-gnu/8.3.3\".\n",
      "\n",
      "\n",
      "Due to MODULEPATH changes, the following have been reloaded:\n",
      "  1) cray-mpich/8.1.18\n",
      "\n",
      "\n",
      "Lmod is automatically replacing \"craype-x86-rome\" with \"craype-x86-milan\".\n",
      "\n",
      "MPICH ERROR [Rank 33] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 33 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 33\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 33\n",
      "MPICH ERROR [Rank 60] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 60 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 60\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 60\n",
      "MPICH ERROR [Rank 110] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 110 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 110\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 110\n",
      "MPICH ERROR [Rank 112] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 112 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 112\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 112\n",
      "MPICH ERROR [Rank 113] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 113 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 113\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 113\n",
      "MPICH ERROR [Rank 114] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 114 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 114\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 114\n",
      "MPICH ERROR [Rank 118] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 118 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 118\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 118\n",
      "MPICH ERROR [Rank 4] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 4 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 4\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 4\n",
      "MPICH ERROR [Rank 5] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 5 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 5\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 5\n",
      "MPICH ERROR [Rank 11] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 11 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11\n",
      "MPICH ERROR [Rank 14] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 14 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 14\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 14\n",
      "MPICH ERROR [Rank 15] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 15 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 15\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 15\n",
      "MPICH ERROR [Rank 22] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 22 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 22\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 22\n",
      "MPICH ERROR [Rank 37] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 37 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 37\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 37\n",
      "MPICH ERROR [Rank 45] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 45 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 45\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 45\n",
      "MPICH ERROR [Rank 48] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 48 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 48\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 48\n",
      "MPICH ERROR [Rank 58] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 58 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 58\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 58\n",
      "MPICH ERROR [Rank 68] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 68 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 68\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 68\n",
      "MPICH ERROR [Rank 70] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 70 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 70\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 70\n",
      "MPICH ERROR [Rank 79] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 79 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 79\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 79\n",
      "MPICH ERROR [Rank 81] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 81 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 81\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 81\n",
      "MPICH ERROR [Rank 82] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 82 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 82\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 82\n",
      "MPICH ERROR [Rank 86] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 86 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 86\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 86\n",
      "MPICH ERROR [Rank 90] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 90 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 90\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 90\n",
      "MPICH ERROR [Rank 92] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 92 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 92\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 92\n",
      "MPICH ERROR [Rank 102] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 102 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 102\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 102\n",
      "MPICH ERROR [Rank 119] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 119 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 119\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 119\n",
      "MPICH ERROR [Rank 127] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 127 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 127\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 127\n",
      "MPICH ERROR [Rank 95] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 95 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 95\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 95\n",
      "MPICH ERROR [Rank 49] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 49 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 49\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 49\n",
      "MPICH ERROR [Rank 74] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 74 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 74\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 74\n",
      "MPICH ERROR [Rank 100] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 100 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 100\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 100\n",
      "MPICH ERROR [Rank 101] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 101 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 101\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 101\n",
      "MPICH ERROR [Rank 125] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 125 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 125\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 125\n",
      "MPICH ERROR [Rank 126] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 126 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 126\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 126\n",
      "MPICH ERROR [Rank 2] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 2 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 2\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 2\n",
      "MPICH ERROR [Rank 6] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 6\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 6\n",
      "MPICH ERROR [Rank 10] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 10 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10\n",
      "MPICH ERROR [Rank 12] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 12 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 12\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 12\n",
      "MPICH ERROR [Rank 19] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 19 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 19\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 19\n",
      "MPICH ERROR [Rank 21] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 21 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 21\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 21\n",
      "MPICH ERROR [Rank 23] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 23 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 23\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 23\n",
      "MPICH ERROR [Rank 25] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 25 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 25\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 25\n",
      "MPICH ERROR [Rank 28] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 28 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 28\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 28\n",
      "MPICH ERROR [Rank 38] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 38 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 38\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 38\n",
      "MPICH ERROR [Rank 39] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 39 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 39\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 39\n",
      "MPICH ERROR [Rank 42] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 42 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 42\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 42\n",
      "MPICH ERROR [Rank 44] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 44 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 44\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 44\n",
      "MPICH ERROR [Rank 50] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 50 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 50\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 50\n",
      "MPICH ERROR [Rank 51] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 51 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 51\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 51\n",
      "MPICH ERROR [Rank 55] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 55 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 55\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 55\n",
      "MPICH ERROR [Rank 62] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 62 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 62\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 62\n",
      "MPICH ERROR [Rank 63] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 63 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 63\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 63\n",
      "MPICH ERROR [Rank 65] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 65 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 65\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 65\n",
      "MPICH ERROR [Rank 67] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 67 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 67\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 67\n",
      "MPICH ERROR [Rank 71] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 71 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 71\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 71\n",
      "MPICH ERROR [Rank 72] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 72 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 72\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 72\n",
      "MPICH ERROR [Rank 75] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 75 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 75\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 75\n",
      "MPICH ERROR [Rank 76] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 76 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 76\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 76\n",
      "MPICH ERROR [Rank 77] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 77 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 77\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 77\n",
      "MPICH ERROR [Rank 85] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 85 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 85\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 85\n",
      "MPICH ERROR [Rank 87] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 87 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 87\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 87\n",
      "MPICH ERROR [Rank 98] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 98 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 98\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 98\n",
      "MPICH ERROR [Rank 99] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 99 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 99\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 99\n",
      "MPICH ERROR [Rank 104] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 104 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 104\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 104\n",
      "MPICH ERROR [Rank 105] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 105 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 105\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 105\n",
      "MPICH ERROR [Rank 108] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 108 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 108\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 108\n",
      "MPICH ERROR [Rank 111] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 111 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 111\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 111\n",
      "MPICH ERROR [Rank 115] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 115 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 115\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 115\n",
      "MPICH ERROR [Rank 116] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 116 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 116\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 116\n",
      "MPICH ERROR [Rank 117] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 117 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 117\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 117\n",
      "MPICH ERROR [Rank 124] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 124 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 124\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 124\n",
      "MPICH ERROR [Rank 0] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n",
      "MPICH ERROR [Rank 3] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 3 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 3\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 3\n",
      "MPICH ERROR [Rank 13] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 13 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 13\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 13\n",
      "MPICH ERROR [Rank 18] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 18 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 18\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 18\n",
      "MPICH ERROR [Rank 24] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 24 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 24\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 24\n",
      "MPICH ERROR [Rank 26] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 26 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 26\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 26\n",
      "MPICH ERROR [Rank 30] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 30 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 30\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 30\n",
      "MPICH ERROR [Rank 53] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 53 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 53\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 53\n",
      "MPICH ERROR [Rank 54] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 54 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 54\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 54\n",
      "MPICH ERROR [Rank 64] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 64 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 64\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 64\n",
      "MPICH ERROR [Rank 80] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 80 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 80\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 80\n",
      "MPICH ERROR [Rank 107] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 107 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 107\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 107\n",
      "MPICH ERROR [Rank 123] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 123 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 123\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 123\n",
      "MPICH ERROR [Rank 32] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 32 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 32\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 32\n",
      "MPICH ERROR [Rank 43] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 43 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 43\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 43\n",
      "MPICH ERROR [Rank 29] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 29 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 29\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 29\n",
      "MPICH ERROR [Rank 66] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 66 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 66\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 66\n",
      "MPICH ERROR [Rank 73] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 73 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 73\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 73\n",
      "MPICH ERROR [Rank 88] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 88 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 88\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 88\n",
      "MPICH ERROR [Rank 7] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 7 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 7\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 7\n",
      "MPICH ERROR [Rank 36] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 36 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 36\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 36\n",
      "MPICH ERROR [Rank 109] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 109 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 109\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 109\n",
      "MPICH ERROR [Rank 122] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 122 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 122\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 122\n",
      "MPICH ERROR [Rank 1] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 1 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1\n",
      "MPICH ERROR [Rank 16] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 16 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 16\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 16\n",
      "MPICH ERROR [Rank 46] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 46 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 46\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 46\n",
      "MPICH ERROR [Rank 69] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 69 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 69\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 69\n",
      "MPICH ERROR [Rank 78] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 78 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 78\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 78\n",
      "MPICH ERROR [Rank 84] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 84 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 84\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 84\n",
      "MPICH ERROR [Rank 9] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 9 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 9\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 9\n",
      "MPICH ERROR [Rank 17] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 17 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 17\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 17\n",
      "MPICH ERROR [Rank 35] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 35 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 35\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 35\n",
      "MPICH ERROR [Rank 57] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 57 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 57\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 57\n",
      "MPICH ERROR [Rank 59] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 59 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 59\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 59\n",
      "MPICH ERROR [Rank 89] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 89 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 89\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 89\n",
      "MPICH ERROR [Rank 91] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 91 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 91\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 91\n",
      "MPICH ERROR [Rank 93] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 93 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 93\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 93\n",
      "MPICH ERROR [Rank 94] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 94 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 94\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 94\n",
      "MPICH ERROR [Rank 103] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 103 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 103\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 103\n",
      "MPICH ERROR [Rank 31] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 31 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 31\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 31\n",
      "MPICH ERROR [Rank 34] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 34 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 34\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 34\n",
      "MPICH ERROR [Rank 40] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 40 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 40\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 40\n",
      "MPICH ERROR [Rank 47] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 47 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 47\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 47\n",
      "MPICH ERROR [Rank 83] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 83 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 83\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 83\n",
      "MPICH ERROR [Rank 106] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 106 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 106\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 106\n",
      "MPICH ERROR [Rank 120] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 120 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 120\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 120\n",
      "MPICH ERROR [Rank 8] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 8 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 8\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 8\n",
      "MPICH ERROR [Rank 121] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 121 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 121\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 121\n",
      "MPICH ERROR [Rank 56] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 56 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 56\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 56\n",
      "MPICH ERROR [Rank 61] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 61 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 61\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 61\n",
      "MPICH ERROR [Rank 27] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 27 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 27\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 27\n",
      "MPICH ERROR [Rank 52] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 52 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 52\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 52\n",
      "MPICH ERROR [Rank 97] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 97 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 97\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 97\n",
      "MPICH ERROR [Rank 20] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 20 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 20\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 20\n",
      "MPICH ERROR [Rank 96] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 96 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 96\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 96\n",
      "MPICH ERROR [Rank 41] [job id 2901031.0] [Thu Feb 16 11:20:10 2023] [nid002232] - Abort(1) (rank 41 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 41\n",
      "\n",
      "aborting job:\n",
      "application called MPI_Abort(MPI_COMM_WORLD, 1) - process 41\n",
      "srun: error: nid002232: tasks 0-127: Exited with exit code 255\n",
      "srun: launch/slurm: _step_signal: Terminating StepId=2901031.0\n",
      "\n",
      "*** 4 LOG MESSAGES:\n",
      "+-> WARNING at 2023-02-16 10:21:14.290634+01:00\n",
      " | key 'symmetries' is not present in raw output dictionary\n",
      "+-> ERROR at 2023-02-16 10:21:14.371420+01:00\n",
      " | ERROR_OUTPUT_STDOUT_INCOMPLETE\n",
      "+-> ERROR at 2023-02-16 10:21:14.385860+01:00\n",
      " | Both the stdout and XML output files could not be read or parsed.\n",
      "+-> WARNING at 2023-02-16 10:21:14.390896+01:00\n",
      " | output parser returned exit code<305>: Both the stdout and XML output files could not be read or parsed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "verdi process report 2318\n",
    "# verdi process report 2310\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure: olivine_LiFePO, Size: 1, # Configs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>li_number</th>\n",
       "      <th>formula</th>\n",
       "      <th>space_group</th>\n",
       "      <th>lithiation</th>\n",
       "      <th>li_mpc</th>\n",
       "      <th>cell_mpc</th>\n",
       "      <th>sg_mpc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Fe4Li4O16P4</td>\n",
       "      <td>Pm</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Fe4Li3O16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Fe4Li3O16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe4Li2O16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe4Li2O16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe4Li2O16P4</td>\n",
       "      <td>Pm</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe4Li2O16P4</td>\n",
       "      <td>Pm</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Fe4LiO16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Fe4LiO16P4</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Fe4O16P4</td>\n",
       "      <td>Pm</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   li_number      formula space_group  lithiation  li_mpc  cell_mpc  sg_mpc\n",
       "0          4  Fe4Li4O16P4          Pm        1.00       1        10       4\n",
       "1          3  Fe4Li3O16P4          P1        0.75       2        10       6\n",
       "2          3  Fe4Li3O16P4          P1        0.75       2        10       6\n",
       "3          2  Fe4Li2O16P4          P1        0.50       4        10       6\n",
       "4          2  Fe4Li2O16P4          P1        0.50       4        10       6\n",
       "5          2  Fe4Li2O16P4          Pm        0.50       4        10       4\n",
       "6          2  Fe4Li2O16P4          Pm        0.50       4        10       4\n",
       "7          1   Fe4LiO16P4          P1        0.25       2        10       6\n",
       "8          1   Fe4LiO16P4          P1        0.25       2        10       6\n",
       "9          0     Fe4O16P4          Pm        0.00       1        10       4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure: olivine_LiFePO, Size: 2, # Configs: 462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>li_number</th>\n",
       "      <th>formula</th>\n",
       "      <th>space_group</th>\n",
       "      <th>lithiation</th>\n",
       "      <th>li_mpc</th>\n",
       "      <th>cell_mpc</th>\n",
       "      <th>sg_mpc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Fe8Li7O32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Fe8Li7O32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Fe8Li6O32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.750</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Fe8Li6O32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.750</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Fe8Li6O32P8</td>\n",
       "      <td>Pm</td>\n",
       "      <td>0.750</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe8Li2O32P8</td>\n",
       "      <td>Pc</td>\n",
       "      <td>0.250</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe8Li2O32P8</td>\n",
       "      <td>Pm</td>\n",
       "      <td>0.250</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2</td>\n",
       "      <td>Fe8Li2O32P8</td>\n",
       "      <td>Pc</td>\n",
       "      <td>0.250</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1</td>\n",
       "      <td>Fe8LiO32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>1</td>\n",
       "      <td>Fe8LiO32P8</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     li_number      formula space_group  lithiation  li_mpc  cell_mpc  sg_mpc\n",
       "0            7  Fe8Li7O32P8          P1       0.875      14        66     378\n",
       "1            7  Fe8Li7O32P8          P1       0.875      14        66     378\n",
       "2            6  Fe8Li6O32P8          P1       0.750      56        66     378\n",
       "3            6  Fe8Li6O32P8          P1       0.750      56        66     378\n",
       "4            6  Fe8Li6O32P8          Pm       0.750      56        66      30\n",
       "..         ...          ...         ...         ...     ...       ...     ...\n",
       "457          2  Fe8Li2O32P8          Pc       0.250      56        66      18\n",
       "458          2  Fe8Li2O32P8          Pm       0.250      56        66      30\n",
       "459          2  Fe8Li2O32P8          Pc       0.250      56        66      18\n",
       "460          1   Fe8LiO32P8          P1       0.125      14        66     378\n",
       "461          1   Fe8LiO32P8          P1       0.125      14        66     378\n",
       "\n",
       "[462 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_columns_sampling = ['li_number', 'formula', 'space_group', 'lithiation', 'li_mpc', 'cell_mpc', 'sg_mpc']\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "for sampling_df_key in sampling_df_keys[:2]:\n",
    "    current_df = sampling_df_dict[sampling_df_key][show_columns_sampling]\n",
    "    print(\"Structure: {}, Size: {}, # Configs: {}\".format(*sampling_df_key, current_df.shape[0]))\n",
    "    current_df\n",
    "    # view(sampling_df_dict[sampling_df_key]['ase_structure'].values)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12969d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "563b968e1044a963233ca0d255c24a54b27faba64327a7678a42d7af4cac992e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
